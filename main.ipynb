{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval project \n",
    "**Authors:** Arduini L., Menchini L., Namaki Ghaneh D., Petruzzella C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ir_datasets\n",
    "!pip install nltk\n",
    "!pip install ir_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "\n",
    "# Load the MS MARCO dataset\n",
    "dataset = ir_datasets.load(\"msmarco-passage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first document in the dataset\n",
    "\n",
    "import random\n",
    "\n",
    "# Initialize a flag to check if a document has been printed\n",
    "document_printed = False\n",
    "\n",
    "# Iterate over the documents in the dataset\n",
    "for doc in dataset.docs_iter():\n",
    "    if not document_printed:\n",
    "        if random.random() < 0.01:  # Adjust the probability as needed\n",
    "            print(doc.text)\n",
    "            document_printed = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "# Compile regex patterns once globally\n",
    "ACRONYM_REGEX = re.compile(r\"(?<!\\w)\\.(?!\\d)\")\n",
    "PUNCTUATION_TRANS = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "# Preload stopwords set\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Initialize stemmer\n",
    "STEMMER = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(s):\n",
    "    # lowercasing\n",
    "    s = s.lower()\n",
    "    \n",
    "    # replace ampersand\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    \n",
    "    # normalize quotes and dashes\n",
    "    s = s.translate(str.maketrans(\"‘’´“”–-\", \"'''\\\"\\\"--\"))\n",
    "    \n",
    "    # remove unnecessary dots in acronyms (but not decimals)\n",
    "    s = ACRONYM_REGEX.sub(\"\", s)\n",
    "    \n",
    "    # remove punctuation\n",
    "    s = s.translate(PUNCTUATION_TRANS)\n",
    "    \n",
    "    # strip and remove extra spaces\n",
    "    s = \" \".join(s.split())\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = s.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    \n",
    "    # stemming\n",
    "    tokens = [STEMMER.stem(t) for t in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def profile(f):\n",
    "    def f_timer(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        ms = (end - start) * 1000\n",
    "        print(f\"{f.__name__} ({ms:.3f} ms)\")\n",
    "        return result\n",
    "    return f_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "@profile\n",
    "def build_index(dataset):\n",
    "    lexicon = {}\n",
    "    doc_index = []\n",
    "    \n",
    "    # Uniamo inv_d e inv_f in un unico dizionario usando defaultdict\n",
    "    inverted_index = defaultdict(lambda: {'docids': [], 'freqs': []})\n",
    "    \n",
    "    termid = 0\n",
    "    total_dl = 0\n",
    "    num_docs = 0\n",
    "    \n",
    "    # Iteriamo sui documenti del dataset\n",
    "    for docid, doc in tqdm(enumerate(dataset.docs_iter()), desc='Indexing', total=dataset.docs_count()):\n",
    "        tokens = preprocess(doc.text)\n",
    "        token_tf = Counter(tokens)  # Frequenze dei termini nel documento\n",
    "        doclen = len(tokens)\n",
    "        total_dl += doclen\n",
    "        num_docs += 1\n",
    "        \n",
    "        # Aggiorniamo l'indice invertito e il lexicon\n",
    "        for token, tf in token_tf.items():\n",
    "            if token not in lexicon:\n",
    "                lexicon[token] = [termid, 0, 0]  # [termid, df, tf]\n",
    "                termid += 1\n",
    "\n",
    "            token_id = lexicon[token][0]\n",
    "            lexicon[token][1] += 1  # Incrementiamo df\n",
    "            lexicon[token][2] += tf  # Incrementiamo tf\n",
    "\n",
    "            inverted_index[token_id]['docids'].append(docid)\n",
    "            inverted_index[token_id]['freqs'].append(tf)\n",
    "\n",
    "        # Inseriamo il documento nell'indice\n",
    "        doc_index.append((str(doc.doc_id), doclen))\n",
    "    \n",
    "    # Statistiche finali\n",
    "    stats = {\n",
    "        'num_docs': num_docs,\n",
    "        'num_terms': len(lexicon),\n",
    "        'num_tokens': total_dl,\n",
    "    }\n",
    "    \n",
    "    return lexicon, inverted_index, doc_index, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex, inv, doc, stats = build_index(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress and save the index components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "with gzip.open('lexicon.pickle.gz', 'wb') as f:\n",
    "  pickle.dump(lex, f)\n",
    "with gzip.open('inverted_file.pickle.gz', 'wb') as f:\n",
    "  pickle.dump(inv, f)\n",
    "with gzip.open('document_index.pickle.gz', 'wb') as f:\n",
    "  pickle.dump(doc, f)\n",
    "with gzip.open('stats.pickle.gz', 'wb') as f:\n",
    "  pickle.dump(stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decompress and load the index components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('lexicon.pickle.gz', 'rb') as f:\n",
    "  lex = pickle.load(f)\n",
    "with gzip.open('inverted_file.pickle.gz', 'rb') as f:\n",
    "  inv = pickle.load(f)\n",
    "with gzip.open('document_index.pickle.gz', 'rb') as f:\n",
    "  doc = pickle.load(f)\n",
    "with gzip.open('stats.pickle.gz', 'rb') as f:\n",
    "  stats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Query processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in dataset.docs_iter()[:3]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trec_dl_2020 = ir_datasets.load(\"msmarco-passage/trec-dl-2020\")\n",
    "for query in trec_dl_2020.queries_iter()[:3]:\n",
    "    print(query) # namedtuple<query_id, text>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ass in list(trec_dl_2020.qrels_iter())[:3]:\n",
    "  print(ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate run file\n",
    "trec_run_list = []\n",
    "for query_id, doc_scores in results.items():\n",
    "    rank = 1\n",
    "    for doc_id, score in doc_scores:\n",
    "        line = f\"{query_id} Q0 {doc_id} {rank} {score} GOODFELLAS\"\n",
    "        trec_run_list.append(line)\n",
    "        rank += 1\n",
    "\n",
    "with open(\"trec_eval_run_file.txt\", \"w\") as f:\n",
    "    for line in trec_run_list:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create format for Trec_Eval\n",
    "qrels_file = []\n",
    "for qrel in trec_dl_2020.qrels_iter():\n",
    "    line = f\"{qrel.query_id} 0 {qrel.doc_id} {qrel.relevance}\"\n",
    "    qrels_file.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trec_eval_qrels_file.txt\", \"w\") as f:\n",
    "    for line in qrels_file:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "qrels = ir_measures.read_trec_qrels('trec_eval_qrels_file.txt')\n",
    "run = ir_measures.read_trec_run('trec_eval_run_file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = ir_measures.P@10, ir_measures.R@1000, ir_measures.AP, ir_measures.nDCG@10\n",
    "results = ir_measures.calc_aggregate(measures, qrels, run)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mircv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
