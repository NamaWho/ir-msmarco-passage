{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval project \n",
    "**Authors:** Arduini L., Menchini L., Namaki Ghaneh D., Petruzzella C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ir_datasets in ./mircv_env/lib/python3.12/site-packages (0.5.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (4.12.3)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (2.5.0)\n",
      "Requirement already satisfied: lxml>=4.5.2 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.1 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (2.1.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.38.0 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (4.66.5)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (4.3.3)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (0.2.5)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (0.1.9)\n",
      "Requirement already satisfied: ijson>=3.1.3 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (3.3.0)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (0.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./mircv_env/lib/python3.12/site-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./mircv_env/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./mircv_env/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./mircv_env/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./mircv_env/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (2024.8.30)\n",
      "Requirement already satisfied: cbor>=1.0.0 in ./mircv_env/lib/python3.12/site-packages (from trec-car-tools>=2.5.4->ir_datasets) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: nltk in ./mircv_env/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in ./mircv_env/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./mircv_env/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./mircv_env/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in ./mircv_env/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ir_datasets\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "\n",
    "# Load the MS MARCO dataset\n",
    "dataset = ir_datasets.load(\"msmarco-passage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rise of Industrial America, 1877-1900. When in 1873 Mark Twain and Charles Dudley Warner entitled their co-authored novel The Gilded Age, they gave the late nineteenth century its popular name. The term reflected the combination of outward wealth and dazzle with inner corruption and poverty.\n"
     ]
    }
   ],
   "source": [
    "# print the first document in the dataset\n",
    "\n",
    "import random\n",
    "\n",
    "# Initialize a flag to check if a document has been printed\n",
    "document_printed = False\n",
    "\n",
    "# Iterate over the documents in the dataset\n",
    "for doc in dataset.docs_iter():\n",
    "    if not document_printed:\n",
    "        if random.random() < 0.01:  # Adjust the probability as needed\n",
    "            print(doc.text)\n",
    "            document_printed = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "# Compile regex patterns once globally\n",
    "ACRONYM_REGEX = re.compile(r\"(?<!\\w)\\.(?!\\d)\")\n",
    "PUNCTUATION_TRANS = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "# Preload stopwords set\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Initialize stemmer\n",
    "STEMMER = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(s):\n",
    "    # lowercasing\n",
    "    s = s.lower()\n",
    "    \n",
    "    # replace ampersand\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    \n",
    "    # normalize quotes and dashes\n",
    "    s = s.translate(str.maketrans(\"‘’´“”–-\", \"'''\\\"\\\"--\"))\n",
    "    \n",
    "    # remove unnecessary dots in acronyms (but not decimals)\n",
    "    s = ACRONYM_REGEX.sub(\"\", s)\n",
    "    \n",
    "    # remove punctuation\n",
    "    s = s.translate(PUNCTUATION_TRANS)\n",
    "    \n",
    "    # strip and remove extra spaces\n",
    "    s = \" \".join(s.split())\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = s.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    \n",
    "    # stemming\n",
    "    tokens = [STEMMER.stem(t) for t in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def profile(f):\n",
    "    def f_timer(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        ms = (end - start) * 1000\n",
    "        print(f\"{f.__name__} ({ms:.3f} ms)\")\n",
    "        return result\n",
    "    return f_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "@profile\n",
    "def build_index(dataset):\n",
    "    lexicon = {}\n",
    "    doc_index = []\n",
    "    \n",
    "    # Uniamo inv_d e inv_f in un unico dizionario usando defaultdict\n",
    "    inverted_index = defaultdict(lambda: {'docids': [], 'freqs': []})\n",
    "    \n",
    "    termid = 0\n",
    "    total_dl = 0\n",
    "    num_docs = 0\n",
    "    \n",
    "    # Iteriamo sui documenti del dataset\n",
    "    for docid, doc in tqdm(enumerate(dataset.docs_iter()), desc='Indexing', total=dataset.docs_count()):\n",
    "        tokens = preprocess(doc.text)\n",
    "        token_tf = Counter(tokens)  # Frequenze dei termini nel documento\n",
    "        doclen = len(tokens)\n",
    "        total_dl += doclen\n",
    "        num_docs += 1\n",
    "        \n",
    "        # Aggiorniamo l'indice invertito e il lexicon\n",
    "        for token, tf in token_tf.items():\n",
    "            if token not in lexicon:\n",
    "                lexicon[token] = [termid, 0, 0]  # [termid, df, tf]\n",
    "                termid += 1\n",
    "\n",
    "            token_id = lexicon[token][0]\n",
    "            lexicon[token][1] += 1  # Incrementiamo df\n",
    "            lexicon[token][2] += tf  # Incrementiamo tf\n",
    "\n",
    "            inverted_index[token_id]['docids'].append(docid)\n",
    "            inverted_index[token_id]['freqs'].append(tf)\n",
    "\n",
    "        # Inseriamo il documento nell'indice\n",
    "        doc_index.append((str(doc.doc_id), doclen))\n",
    "    \n",
    "    # Statistiche finali\n",
    "    stats = {\n",
    "        'num_docs': num_docs,\n",
    "        'num_terms': len(lexicon),\n",
    "        'num_tokens': total_dl,\n",
    "    }\n",
    "    \n",
    "    return lexicon, inverted_index, doc_index, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing: 100%|██████████| 8841823/8841823 [1:19:50<00:00, 1845.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_index (4790183.575 ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lex, inv, doc, stats = build_index(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress and save the index components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'build_index.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m   pickle\u001b[38;5;241m.\u001b[39mdump(lex, f)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minverted_file.pickle.gz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 7\u001b[0m   \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43minv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument_index.pickle.gz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m   pickle\u001b[38;5;241m.\u001b[39mdump(doc, f)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'build_index.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "with gzip.open('lexicon.pickle.gz', 'wb') as f:\n",
    "  pickle.dump(lex, f)\n",
    "with gzip.open('inverted_file.pickle.gz', 'wb') as f:\n",
    "  pickle.dump(inv, f)\n",
    "with gzip.open('document_index.pickle.gz', 'wb') as f:\n",
    "  pickle.dump(doc, f)\n",
    "with gzip.open('stats.pickle.gz', 'wb') as f:\n",
    "  pickle.dump(stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decompress and load the index components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('lexicon.pickle.gz', 'rb') as f:\n",
    "  lex = pickle.load(f)\n",
    "with gzip.open('inverted_file.pickle.gz', 'rb') as f:\n",
    "  inv = pickle.load(f)\n",
    "with gzip.open('document_index.pickle.gz', 'rb') as f:\n",
    "  doc = pickle.load(f)\n",
    "with gzip.open('stats.pickle.gz', 'rb') as f:\n",
    "  stats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Query processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mircv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
