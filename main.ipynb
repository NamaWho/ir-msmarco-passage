{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval project \n",
    "**Authors:** Arduini L., Menchini L., Namaki Ghaneh D., Petruzzella C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ir_datasets in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (0.5.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (4.12.3)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (2.5.0)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (2.1.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.38.0 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (4.66.5)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (4.3.3)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (0.2.5)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (0.1.9)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (3.3.0)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_datasets) (0.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (2024.8.30)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from trec-car-tools>=2.5.4->ir_datasets) (1.0.0)\n",
      "Requirement already satisfied: nltk in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: ir_measures in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (0.3.3)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.5 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_measures) (0.5.6)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from ir_measures) (1.0.12)\n",
      "Requirement already satisfied: numpy in /home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages (from cwl-eval>=1.0.10->ir_measures) (2.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ir_datasets\n",
    "!pip install nltk\n",
    "!pip install ir_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "\n",
    "# Load the MS MARCO dataset\n",
    "dataset = ir_datasets.load(\"msmarco-passage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For example, the pressure of electromagnetic radiation on an object derives from the transfer of photon momentum per unit time and unit area to that object, since pressure is force per unit area and force is the change in momentum per unit time.\n"
     ]
    }
   ],
   "source": [
    "# print the first document in the dataset\n",
    "\n",
    "import random\n",
    "\n",
    "# Initialize a flag to check if a document has been printed\n",
    "document_printed = False\n",
    "\n",
    "# Iterate over the documents in the dataset\n",
    "for doc in dataset.docs_iter():\n",
    "    if not document_printed:\n",
    "        if random.random() < 0.01:  # Adjust the probability as needed\n",
    "            print(doc.text)\n",
    "            document_printed = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "# Compile regex patterns once globally\n",
    "ACRONYM_REGEX = re.compile(r\"(?<!\\w)\\.(?!\\d)\")\n",
    "PUNCTUATION_TRANS = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "# Preload stopwords set\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Initialize stemmer\n",
    "STEMMER = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(s):\n",
    "    # lowercasing\n",
    "    s = s.lower()\n",
    "    \n",
    "    # replace ampersand\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    \n",
    "    # normalize quotes and dashes\n",
    "    s = s.translate(str.maketrans(\"‘’´“”–-\", \"'''\\\"\\\"--\"))\n",
    "    \n",
    "    # remove unnecessary dots in acronyms (but not decimals)\n",
    "    s = ACRONYM_REGEX.sub(\"\", s)\n",
    "    \n",
    "    # remove punctuation\n",
    "    s = s.translate(PUNCTUATION_TRANS)\n",
    "    \n",
    "    # strip and remove extra spaces\n",
    "    s = \" \".join(s.split())\n",
    "    \n",
    "    # tokenize\n",
    "    tokens = s.split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    \n",
    "    # stemming\n",
    "    tokens = [STEMMER.stem(t) for t in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def profile(f):\n",
    "    def f_timer(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        ms = (end - start) * 1000\n",
    "        print(f\"{f.__name__} ({ms:.3f} ms)\")\n",
    "        return result\n",
    "    return f_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aegis/Desktop/ae/esami_in_preparazione/ComputerVision/pratical/mircv_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@profile\n",
    "def build_index(dataset):\n",
    "    lexicon = {}\n",
    "    doc_index = [] \n",
    "    inv_d, inv_f = {}, {}\n",
    "    termid = 0\n",
    "\n",
    "    num_docs = 0\n",
    "    total_dl = 0\n",
    "    total_toks = 0\n",
    "    for docid, doc in tqdm(enumerate(dataset.docs_iter()), desc='Indexing', total=dataset.docs_count()):\n",
    "        tokens = preprocess(doc.text)\n",
    "        token_tf = Counter(tokens)\n",
    "        for token, tf in token_tf.items():\n",
    "            if token not in lexicon:\n",
    "                lexicon[token] = [termid, 0, 0]\n",
    "                inv_d[termid], inv_f[termid] =  [], []\n",
    "                termid += 1\n",
    "            token_id = lexicon[token][0]\n",
    "            inv_d[token_id].append(docid)\n",
    "            inv_f[token_id].append(tf)\n",
    "            lexicon[token][1] += 1\n",
    "            lexicon[token][2] += tf\n",
    "        doclen = len(tokens)\n",
    "        doc_index.append((str(doc.doc_id), doclen))\n",
    "        total_dl += doclen                         \n",
    "        num_docs += 1\n",
    "        \n",
    "\n",
    "    stats = {\n",
    "        'num_docs': 1 + docid,\n",
    "        'num_terms': len(lexicon),\n",
    "        'num_tokens': total_dl,\n",
    "    }\n",
    "    return lexicon, {'docids': inv_d, 'freqs': inv_f}, doc_index, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress and save the index components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlexicon.pickle.gz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m   pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mlex\u001b[49m, f)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minverted_file.pickle.gz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m   pickle\u001b[38;5;241m.\u001b[39mdump(inv, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lex' is not defined"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#\n",
    "# check if files exists in the current directory, if not we build the index and we save the files\n",
    "#\n",
    "\n",
    "files_to_check = ['lexicon.pickle.gz', 'inverted_file.pickle.gz', 'document_index.pickle.gz', 'stats.pickle.gz']\n",
    "if all(os.path.exists(file) for file in files_to_check):\n",
    "  print(\"All files already exist.\")\n",
    "  with gzip.open('lexicon.pickle.gz', 'rb') as f:\n",
    "    lex = pickle.load(f)\n",
    "  with gzip.open('inverted_file.pickle.gz', 'rb') as f:\n",
    "    inv = pickle.load(f)\n",
    "  with gzip.open('document_index.pickle.gz', 'rb') as f:\n",
    "    doc = pickle.load(f)\n",
    "  with gzip.open('stats.pickle.gz', 'rb') as f:\n",
    "    stats = pickle.load(f)\n",
    "else:\n",
    "  lex, inv, doc, stats = build_index(dataset)\n",
    "  with gzip.open('lexicon.pickle.gz', 'wb') as f:\n",
    "    pickle.dump(lex, f) \n",
    "  with gzip.open('inverted_file.pickle.gz', 'wb') as f:\n",
    "    pickle.dump(inv, f)\n",
    "  with gzip.open('document_index.pickle.gz', 'wb') as f:\n",
    "    pickle.dump(doc, f)\n",
    "  with gzip.open('stats.pickle.gz', 'wb') as f:\n",
    "    pickle.dump(stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decompress and load the index components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Query processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenericDoc(doc_id='1', text='compact memories have flexible capacities  a digital data storage\\nsystem with capacity up to bits and random and or sequential access\\nis described\\n')\n",
      "GenericDoc(doc_id='2', text='an electronic analogue computer for solving systems of linear equations\\nmathematical derivation of the operating principle and stability\\nconditions for a computer consisting of amplifiers\\n')\n",
      "GenericDoc(doc_id='3', text='electronic coordinate transformer  circuit details are given for\\nthe construction of an electronic calculating unit which enables\\nthe polar coordinates of a vector modulus and cosine or sine of the\\nargument to be derived from those of a rectangular system of axes\\n')\n"
     ]
    }
   ],
   "source": [
    "for doc in dataset.docs_iter()[:3]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenericQuery(query_id='1030303', text='who is aziz hashim')\n",
      "GenericQuery(query_id='1037496', text='who is rep scalise?')\n",
      "GenericQuery(query_id='1043135', text='who killed nicholas ii of russia')\n"
     ]
    }
   ],
   "source": [
    "trec_dl_2020 = ir_datasets.load(\"msmarco-passage/trec-dl-2020\")\n",
    "for query in trec_dl_2020.queries_iter()[:3]:\n",
    "    print(query) # namedtuple<query_id, text>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrecQrel(query_id='23849', doc_id='1020327', relevance=2, iteration='0')\n",
      "TrecQrel(query_id='23849', doc_id='1034183', relevance=3, iteration='0')\n",
      "TrecQrel(query_id='23849', doc_id='1120730', relevance=0, iteration='0')\n"
     ]
    }
   ],
   "source": [
    "for ass in list(trec_dl_2020.qrels_iter())[:3]:\n",
    "  print(ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate run file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trec_run_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m query_id, doc_scores \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      4\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc_id, score \u001b[38;5;129;01min\u001b[39;00m doc_scores:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate run file\n",
    "trec_run_list = []\n",
    "for query_id, doc_scores in results.items():\n",
    "    rank = 1\n",
    "    for doc_id, score in doc_scores:\n",
    "        line = f\"{query_id} Q0 {doc_id} {rank} {score} GOODFELLAS\"\n",
    "        trec_run_list.append(line)\n",
    "        rank += 1\n",
    "\n",
    "with open(\"trec_eval_run_file.txt\", \"w\") as f:\n",
    "    for line in trec_run_list:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create format for Trec_Eval\n",
    "qrels_file = []\n",
    "for qrel in trec_dl_2020.qrels_iter():\n",
    "    line = f\"{qrel.query_id} 0 {qrel.doc_id} {qrel.relevance}\"\n",
    "    qrels_file.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trec_eval_qrels_file.txt\", \"w\") as f:\n",
    "    for line in qrels_file:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "qrels = ir_measures.read_trec_qrels('trec_eval_qrels_file.txt')\n",
    "run = ir_measures.read_trec_run('trec_eval_run_file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = ir_measures.P@10, ir_measures.R@1000, ir_measures.AP, ir_measures.nDCG@10\n",
    "results = ir_measures.calc_aggregate(measures, qrels, run)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mircv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
