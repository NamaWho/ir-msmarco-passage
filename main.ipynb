{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MpRz48IHcdf"
   },
   "source": [
    "# Information Retrieval project\n",
    "**Authors:** Arduini L., Menchini L., Namaki Ghaneh D., Petruzzella C.\n",
    "\n",
    "**Dataset:** The chosen dataset is MSMARCO Passage dataset ()\n",
    "\n",
    "**Evaluation:** For evaluation the trec-2020-dl dataset has been used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup environment and dependencies\n",
    "This section ensures that all necessary packages are installed and loaded.\n",
    "\n",
    "**Note:** The project uses `ir_datasets`, `nltk`, and `ir_measures`, along with several utilities for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14506,
     "status": "ok",
     "timestamp": 1729868591658,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "vpWABC3lHcdg",
    "outputId": "81d13858-5942-4e0b-a3bc-ed65856f81bf"
   },
   "outputs": [],
   "source": [
    "!pip install ir_datasets\n",
    "!pip install nltk\n",
    "!pip install ir_measures\n",
    "!pip install PyStemmer\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import ir_measures\n",
    "from ir_measures import *\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import heapq\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the dataset\n",
    "\n",
    "This notebook will load the MS MARCO Passage dataset, a standard dataset for Information Retrieval tasks.\n",
    "It contains passages from various sources and is used to train and evaluate retrieval models.\n",
    "\n",
    "For testing purposes, also the Vaswani dataset will be used in a development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1729868591658,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "U79MpmqyHcdi"
   },
   "outputs": [],
   "source": [
    "# ------- Production Environment -------\n",
    "dataset = ir_datasets.load(\"msmarco-passage\")\n",
    "# ---------------------------------------\n",
    "\n",
    "# ------- Development Environment -------\n",
    "# dataset = ir_datasets.load(\"vaswani\")\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing text data\n",
    "This section defines functions for text preprocessing. Preprocessing steps include:\n",
    "- Lowercasing\n",
    "- Replacing symbols and punctuations\n",
    "- Removing stopwords\n",
    "- Stemming tokens\n",
    "\n",
    "The goal is to normalize text data for effective retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1729868591658,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "Q1Hq10LVHcdj"
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import Stemmer\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "# ------- Pre Initialization -------\n",
    "# 1. Compile regex patterns once globally\n",
    "# 2. Preload stopwords set\n",
    "# 3. Initialize stemmer\n",
    "\n",
    "ACRONYM_REGEX = re.compile(r\"(?<!\\w)\\.(?!\\d)\")\n",
    "PUNCTUATION_TRANS = str.maketrans(\"\", \"\", string.punctuation)\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "STEMMER = Stemmer.Stemmer('english')\n",
    "\n",
    "# Define a cached function to stem individual words\n",
    "@lru_cache(maxsize=1000)\n",
    "def stem(word):\n",
    "    return STEMMER.stemWord(word)\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "def preprocess(s):\n",
    "    \"\"\"\n",
    "    Preprocess a string for indexing or querying.\n",
    "\n",
    "    Args:\n",
    "        s: The input string.\n",
    "\n",
    "    Returns:\n",
    "        A list of preprocessed tokens.    \n",
    "    \"\"\"\n",
    "\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    # normalize quotes and dashes\n",
    "    s = s.translate(str.maketrans(\"‘’´“”–-\", \"'''\\\"\\\"--\"))\n",
    "    # remove unnecessary dots in acronyms (but not decimals)\n",
    "    s = ACRONYM_REGEX.sub(\"\", s)\n",
    "    # remove punctuation\n",
    "    s = s.translate(PUNCTUATION_TRANS)\n",
    "    # strip and remove extra spaces\n",
    "    s = \" \".join(s.split())\n",
    "\n",
    "    tokens = s.split()\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS]\n",
    "    # Apply cached stemming function\n",
    "    # tokens = [stem(t) for t in tokens]\n",
    "    tokens = STEMMER.stemWords(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1729868591658,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "Bkuc9p8MHcdk"
   },
   "outputs": [],
   "source": [
    "def profile(f):\n",
    "    \"\"\"\n",
    "    A decorator that prints the runtime of the decorated function.\n",
    "\n",
    "    Args:\n",
    "        f: The function to profile.\n",
    "\n",
    "    Returns:\n",
    "        The profiled function.\n",
    "    \"\"\"\n",
    "    def f_timer(*args, **kwargs):\n",
    "        \"\"\"\n",
    "        The profiled function.\n",
    "        \n",
    "        Args:\n",
    "            *args: The arguments to the function.\n",
    "            **kwargs: The keyword arguments to the function.\n",
    "            \n",
    "        Returns:\n",
    "            The result of the function.\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        ms = (end - start) * 1000\n",
    "        print(f\"{f.__name__} ({ms:.3f} ms)\")\n",
    "        return result\n",
    "    return f_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building the inverted index\n",
    "We create an inverted index to store terms with their respective document IDs and term frequencies.\n",
    "The `build_index` function processes the dataset and constructs a structure that enables efficient term-based searching across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1729868591658,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "-Ugz6FpdHcdl"
   },
   "outputs": [],
   "source": [
    "@profile\n",
    "def build_index(dataset):\n",
    "    \"\"\"\n",
    "    Build an inverted index from a dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: The dataset to index.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of:\n",
    "        - The lexicon, a dictionary mapping terms to term IDs and document frequencies.\n",
    "        - The inverted index, a dictionary mapping term IDs to lists of document IDs and frequencies.\n",
    "        - The document index, a list of document IDs and document lengths.\n",
    "        - The index statistics, a dictionary of statistics.\n",
    "    \"\"\"\n",
    "    lexicon = {}\n",
    "    doc_index = []\n",
    "    inv_d, inv_f = {}, {}\n",
    "    termid = 0\n",
    "\n",
    "    num_docs = 0\n",
    "    total_dl = 0\n",
    "    total_toks = 0\n",
    "    for docid, doc in tqdm(enumerate(dataset.docs_iter()), desc='Indexing', total=dataset.docs_count()):\n",
    "        tokens = preprocess(doc.text)\n",
    "        token_tf = Counter(tokens)\n",
    "        for token, tf in token_tf.items():\n",
    "            if token not in lexicon:\n",
    "                lexicon[token] = [termid, 0, 0]\n",
    "                inv_d[termid], inv_f[termid] =  [], []\n",
    "                termid += 1\n",
    "            token_id = lexicon[token][0]\n",
    "            inv_d[token_id].append(docid)\n",
    "            inv_f[token_id].append(tf)\n",
    "            lexicon[token][1] += 1\n",
    "            lexicon[token][2] += tf\n",
    "        doclen = len(tokens)\n",
    "        doc_index.append((str(doc.doc_id), doclen))\n",
    "        total_dl += doclen\n",
    "        num_docs += 1\n",
    "\n",
    "\n",
    "    stats = {\n",
    "        'num_docs': 1 + docid,\n",
    "        'num_terms': len(lexicon),\n",
    "        'num_tokens': total_dl,\n",
    "    }\n",
    "    return lexicon, {'docids': inv_d, 'freqs': inv_f}, doc_index, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1729868591658,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "JhOE3PCeHcdl",
    "outputId": "e1ec7df6-a039-4979-a829-3d541ec5ab21"
   },
   "outputs": [],
   "source": [
    "lex, inv, doc, stats = None, None, None, None\n",
    "\n",
    "files = ['lexicon.pickle.gz', 'inverted_file.pickle.gz', 'document_index.pickle.gz', 'stats.pickle.gz']\n",
    "if all(os.path.exists(file) for file in files):\n",
    "    print(\"All files already exist.\")\n",
    "    \n",
    "    for file, var_name in zip(files, ['lex', 'inv', 'doc', 'stats']):\n",
    "        try:\n",
    "            if os.path.getsize(file) > 0:  # Verifica se il file non è vuoto\n",
    "                with gzip.open(file, 'rb') as f:\n",
    "                    globals()[var_name] = pickle.load(f)\n",
    "            else:\n",
    "                print(f\"Warning: {file} is empty.\")\n",
    "        except EOFError:\n",
    "            print(f\"Error: {file} is corrupted or incomplete. Rebuilding the index.\")\n",
    "            lex, inv, doc, stats = build_index(dataset)\n",
    "            break\n",
    "else:\n",
    "    # Se i file non esistono o sono corrotti, ricostruisci l'indice\n",
    "    lex, inv, doc, stats = build_index(dataset)\n",
    "\n",
    "    # Salva nuovamente i dati nei file compressi solo se necessario\n",
    "    for data, file in zip([lex, inv, doc, stats], files):\n",
    "      with gzip.open(file, 'wb') as f:\n",
    "        print(f\"Saving {file}...\")\n",
    "        pickle.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729868591658,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "4BRtzyxlHcdm"
   },
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    \"\"\"\n",
    "    A simple inverted index class.\n",
    "    \n",
    "    Attributes:\n",
    "        lexicon: The lexicon.\n",
    "        inv: The inverted index.\n",
    "        doc: The document index.\n",
    "        stats: The index statistics.\n",
    "        \n",
    "    Methods:\n",
    "        num_docs: Get the number of documents in the index.\n",
    "        get_posting: Get a posting list iterator for a term.\n",
    "        get_termids: Get the term IDs for a list of tokens.\n",
    "        get_postings: Get the posting list iterators for a list of term IDs.\n",
    "        \n",
    "    Inner class:\n",
    "        PostingListIterator: An iterator over a posting list.\n",
    "    \"\"\"\n",
    "\n",
    "    class PostingListIterator:\n",
    "        \"\"\"\n",
    "        An iterator over a posting list.\n",
    "\n",
    "        Attributes:\n",
    "            docids: The list of document IDs.\n",
    "            freqs: The list of term frequencies.\n",
    "            pos: The current position in the posting list.\n",
    "            doc: The document index.\n",
    "\n",
    "        Methods:\n",
    "            docid: Get the current document ID.\n",
    "            score: Get the current document score.\n",
    "            next: Move to the next document.\n",
    "            is_end_list: Check if the iterator is at the end of the list.\n",
    "            len: Get the length of the posting list.\n",
    "        \"\"\"\n",
    "        def __init__(self, docids, freqs, doc):\n",
    "            self.docids = docids\n",
    "            self.freqs = freqs\n",
    "            self.pos = 0\n",
    "            self.doc = doc\n",
    "\n",
    "        def docid(self):\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "            return self.docids[self.pos]\n",
    "\n",
    "        def score(self):\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "            return self.freqs[self.pos]/self.doc[self.docid()][1]\n",
    "\n",
    "        def next(self, target = None):\n",
    "            if not target:\n",
    "                if not self.is_end_list():\n",
    "                    self.pos += 1\n",
    "            else:\n",
    "                if target > self.docid():\n",
    "                    try:\n",
    "                        self.pos = self.docids.index(target, self.pos)\n",
    "                    except ValueError:\n",
    "                        self.pos = len(self.docids)\n",
    "\n",
    "        def is_end_list(self):\n",
    "            return self.pos == len(self.docids)\n",
    "\n",
    "\n",
    "        def len(self):\n",
    "            return len(self.docids)\n",
    "\n",
    "\n",
    "    def __init__(self, lex, inv, doc, stats):\n",
    "        self.lexicon = lex\n",
    "        self.inv = inv\n",
    "        self.doc = doc\n",
    "        self.stat = stats\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.stats['num_docs']\n",
    "\n",
    "    def get_posting(self, termid):\n",
    "        return InvertedIndex.PostingListIterator(self.inv['docids'][termid], self.inv['freqs'][termid], self.doc)\n",
    "\n",
    "    def get_termids(self, tokens):\n",
    "        return [self.lexicon[token][0] for token in tokens if token in self.lexicon]\n",
    "\n",
    "    def get_postings(self, termids):\n",
    "        return [self.get_posting(termid) for termid in termids]\n",
    "    \n",
    "inv_index = InvertedIndex(lex, inv, doc, stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XA-LFkjhHcdm"
   },
   "source": [
    "# 4. Query processing\n",
    "This section implements the Query Processing task, aiming to rank documents by relevance to a given query using the BM25 and TF-IDF scoring algorithm with Document-at-a-Time (DAAT) and Term-at-a-Time (TAAT) approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1729868591659,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "Edu7ZtPfHcdn"
   },
   "outputs": [],
   "source": [
    "# ------- Production Environment --------\n",
    "trec_dl_2020 = ir_datasets.load(\"msmarco-passage/trec-dl-2020\")\n",
    "# ---------------------------------------\n",
    "\n",
    "# ------- Development Environment -------\n",
    "# trec_dl_2020 = ir_datasets.load(\"vaswani\")\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729868591659,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "ywDp7MMVHcdn"
   },
   "outputs": [],
   "source": [
    "class TopQueue:\n",
    "    \"\"\"\n",
    "    A simple top-k queue class.\n",
    "    \n",
    "    Attributes:\n",
    "        queue: The priority queue.\n",
    "        k: The maximum number of items in the queue.\n",
    "        threshold: The minimum score threshold.\n",
    "        \n",
    "    Methods:\n",
    "        size: Get the number of items in the queue.\n",
    "        would_enter: Check if a score would enter the queue.\n",
    "        clear: Clear the queue.\n",
    "        insert: Insert a document into the queue.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=10, threshold=0.0):\n",
    "        self.queue = []\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.queue)\n",
    "\n",
    "    def would_enter(self, score):\n",
    "        return score > self.threshold\n",
    "\n",
    "    def clear(self, new_threshold=None):\n",
    "        self.queue = []\n",
    "        if new_threshold:\n",
    "            self.threshold = new_threshold\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<{self.size()} items, th={self.threshold} {self.queue}'\n",
    "\n",
    "    def insert(self, docid, score):\n",
    "        if score > self.threshold:\n",
    "            if self.size() >= self.k:\n",
    "                heapq.heapreplace(self.queue, (score, docid))\n",
    "            else:\n",
    "                heapq.heappush(self.queue, (score, docid))\n",
    "            if self.size() >= self.k:\n",
    "                self.threshold = max(self.threshold, self.queue[0][0])\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729868591659,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "l6naDw7fHcdn"
   },
   "outputs": [],
   "source": [
    "# Average document length\n",
    "avg_dl = inv_index.stat['num_tokens'] / inv_index.stat['num_docs']\n",
    "# Number of documents\n",
    "N = inv_index.stat['num_docs']\n",
    "\n",
    "def bm25(tf, df, dl, k1=1.5, b=0.75):\n",
    "    \"\"\"\n",
    "    Compute the BM25 score.\n",
    "\n",
    "    Args:\n",
    "        tf: The term frequency.\n",
    "        df: The document frequency.\n",
    "        dl: The document length.\n",
    "        k1: The k1 parameter.\n",
    "        b: The b parameter.\n",
    "\n",
    "    Returns:\n",
    "        The BM25 score.\n",
    "    \"\"\"\n",
    "    idf = math.log(1 + (N - df + 0.5) / (df + 0.5))\n",
    "    term_frequency_component = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (dl / avg_dl)))\n",
    "    return idf * term_frequency_component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 DAAT with BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729868591659,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "K3NbYKX2Hcdn"
   },
   "outputs": [],
   "source": [
    "# Calculate document lengths\n",
    "doc_lengths = defaultdict(int)\n",
    "for docid, doc_len in inv_index.doc:\n",
    "    doc_lengths[docid] = doc_len\n",
    "\n",
    "def min_docid(postings):\n",
    "    \"\"\"\n",
    "    Get the minimum document ID from a list of posting list iterators.\n",
    "    \n",
    "    Args:\n",
    "        postings: The list of posting list iterators.\n",
    "        \n",
    "    Returns:\n",
    "        The minimum document ID.\n",
    "    \"\"\"\n",
    "    min_docid = math.inf\n",
    "    for p in postings:\n",
    "        if not p.is_end_list():\n",
    "            min_docid = min(p.docid(), min_docid)\n",
    "    return min_docid\n",
    "\n",
    "def daat_bm25(postings, k=10):\n",
    "    \"\"\"\n",
    "    Perform a document-at-a-time (DAAT) scoring using BM25.\n",
    "\n",
    "    Args:\n",
    "        postings: The list of posting list iterators.\n",
    "        k: The maximum number of documents to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        A list of (docid, score) tuples.\n",
    "    \"\"\"\n",
    "    top = TopQueue(k)\n",
    "    current_docid = min_docid(postings)\n",
    "\n",
    "    while current_docid != math.inf:\n",
    "        score = 0\n",
    "        next_docid = math.inf\n",
    "\n",
    "        for posting in postings:\n",
    "            if posting.docid() == current_docid:\n",
    "                tf = posting.freqs[posting.pos]\n",
    "                df = posting.len()\n",
    "                dl = doc_lengths[current_docid]\n",
    "\n",
    "                score += bm25(tf, df, dl)\n",
    "\n",
    "                posting.next()\n",
    "            if not posting.is_end_list():\n",
    "                next_docid = min(next_docid, posting.docid())\n",
    "\n",
    "        top.insert(current_docid, score)\n",
    "        current_docid = next_docid\n",
    "\n",
    "    return sorted(top.queue, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9Eub_uGHcdo"
   },
   "source": [
    "### 4.1.2 TAAT with BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729868591659,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "P85FZGrxHcdo"
   },
   "outputs": [],
   "source": [
    "def taat_bm25(postings, k=10):\n",
    "    \"\"\"\n",
    "    Perform a term-at-a-time (TAAT) scoring using BM25.\n",
    "\n",
    "    Args:\n",
    "        postings: The list of posting list iterators.\n",
    "        k: The maximum number of documents to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        A list of (docid, score) tuples.\n",
    "    \"\"\"\n",
    "    A = defaultdict(float)\n",
    "\n",
    "    for posting in postings:\n",
    "        current_docid = posting.docid()\n",
    "\n",
    "        df = posting.len()\n",
    "\n",
    "        while current_docid != math.inf:\n",
    "            tf = posting.freqs[posting.pos]\n",
    "            dl = doc_lengths[current_docid]\n",
    "\n",
    "            score = bm25(tf, df, dl)\n",
    "            A[current_docid] += score\n",
    "\n",
    "            posting.next()\n",
    "            current_docid = posting.docid()\n",
    "\n",
    "    top = TopQueue(k)\n",
    "    for docid, score in A.items():\n",
    "        top.insert(docid, score)\n",
    "\n",
    "    return sorted(top.queue, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_score(tf, df, N):\n",
    "    \"\"\"\n",
    "    Compute the TF-IDF score.\n",
    "    \n",
    "    Args:\n",
    "        tf: The term frequency.\n",
    "        df: The document frequency.\n",
    "        N: The number of documents.\n",
    "        \n",
    "    Returns:\n",
    "        The TF-IDF score.\n",
    "    \"\"\"\n",
    "    idf = math.log(N / df)\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 DAAT with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daat_tfidf(postings, k=10):\n",
    "    \"\"\"\n",
    "    Perform a document-at-a-time (DAAT) scoring using TF-IDF.\n",
    "\n",
    "    Args:\n",
    "        postings: The list of posting list iterators.\n",
    "        k: The maximum number of documents to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        A list of (docid, score) tuples.\n",
    "    \"\"\"\n",
    "    top = TopQueue(k)\n",
    "    current_docid = min_docid(postings)\n",
    "\n",
    "    while current_docid != math.inf:\n",
    "        score = 0\n",
    "        next_docid = math.inf\n",
    "\n",
    "        for posting in postings:\n",
    "            if posting.docid() == current_docid:\n",
    "                tf = posting.freqs[posting.pos]\n",
    "                df = posting.len() \n",
    "                \n",
    "                score += tfidf_score(tf, df, N)\n",
    "                \n",
    "                posting.next()\n",
    "            if not posting.is_end_list():\n",
    "                next_docid = min(next_docid, posting.docid())\n",
    "\n",
    "        top.insert(current_docid, score)\n",
    "        current_docid = next_docid\n",
    "\n",
    "    return sorted(top.queue, reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 TAAT with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taat_tfidf(postings, k=10):\n",
    "    \"\"\"\n",
    "    Perform a term-at-a-time (TAAT) scoring using TF-IDF.\n",
    "\n",
    "    Args:\n",
    "        postings: The list of posting list iterators.\n",
    "        k: The maximum number of documents to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        A list of (docid, score) tuples.\n",
    "    \"\"\"\n",
    "    A = defaultdict(float)\n",
    "    \n",
    "    for posting in postings:\n",
    "        current_docid = posting.docid()\n",
    "        \n",
    "        df = posting.len()\n",
    "        \n",
    "        while current_docid != math.inf:\n",
    "            tf = posting.freqs[posting.pos]\n",
    "            \n",
    "            score = tfidf_score(tf, df, N)\n",
    "            A[current_docid] += score\n",
    "            \n",
    "            posting.next()\n",
    "            current_docid = posting.docid()\n",
    "    \n",
    "    top = TopQueue(k)\n",
    "    for docid, score in A.items():\n",
    "        top.insert(docid, score)\n",
    "\n",
    "    return sorted(top.queue, reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1729868591659,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "EmKXPJvBHcdo"
   },
   "outputs": [],
   "source": [
    "@profile\n",
    "def query_processing(queries_iter, fn):\n",
    "    \"\"\"\n",
    "    Process a list of queries using a scoring function.\n",
    "\n",
    "    Args:\n",
    "        queries_iter: The list of queries.\n",
    "        fn: The scoring function.\n",
    "    \n",
    "    Returns:\n",
    "        A list of query results.\n",
    "    \"\"\"\n",
    "    \n",
    "    res = []\n",
    "    for q in queries_iter:\n",
    "        query = preprocess(q.text)\n",
    "        termids = inv_index.get_termids(query)\n",
    "        postings = inv_index.get_postings(termids)\n",
    "        res.append({'query_id': q.query_id, 'scores': fn(postings)})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5432,
     "status": "ok",
     "timestamp": 1729868597085,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "om92ICN-Hcdo",
    "outputId": "328f5039-947c-4ca1-e91f-c042989a9d89"
   },
   "outputs": [],
   "source": [
    "print(query_processing(trec_dl_2020.queries_iter(), daat_bm25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2638,
     "status": "ok",
     "timestamp": 1729868599720,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "UIn-FsL5Hcdo",
    "outputId": "67721fdd-d506-42d1-e831-019385d56f93"
   },
   "outputs": [],
   "source": [
    "bm25_results = query_processing(trec_dl_2020.queries_iter(), taat_bm25)\n",
    "print(bm25_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_processing(trec_dl_2020.queries_iter(), daat_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_results = query_processing(trec_dl_2020.queries_iter(), taat_tfidf)\n",
    "print(tfidf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpb3m8ELHcdo"
   },
   "source": [
    "# 5. Evaluation with TREC-style measures\n",
    "To evaluate retrieval performance, we use the TREC evaluation method with `ir_measures`.\n",
    "\n",
    "This section generates a run file and QRELs for the TREC evaluation tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1729868599720,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "i_cJcwV1Hcdo",
    "outputId": "99944a67-52d8-4ae2-9a7e-8dfce6280e7a"
   },
   "outputs": [],
   "source": [
    "for query in list(trec_dl_2020.queries_iter())[:3]:\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1729868599720,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "GhrquNSbHcdp",
    "outputId": "3a88f667-f2c1-424f-99cd-796a92e6d75c"
   },
   "outputs": [],
   "source": [
    "for ass in list(trec_dl_2020.qrels_iter())[:3]:\n",
    "  print(ass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Run File generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 642,
     "status": "ok",
     "timestamp": 1729868790072,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "HT-n8O0aHcdp",
    "outputId": "75f42b92-deab-47c9-fd38-7353d01e5324"
   },
   "outputs": [],
   "source": [
    "def generate_run(results):\n",
    "    trec_run_list = []\n",
    "    for doc_scores in results:\n",
    "        rank = 1\n",
    "        query_id = doc_scores['query_id']\n",
    "        scores = doc_scores['scores']\n",
    "\n",
    "        for score, doc_id in scores:\n",
    "            line = f\"{query_id} Q0 {doc_id} {rank} {score} GOODFELLAS\"\n",
    "            trec_run_list.append(line)\n",
    "            rank += 1\n",
    "    \n",
    "    return trec_run_list\n",
    "\n",
    "trec_bm25_run_list = generate_run(bm25_results)\n",
    "trec_tfidf_run_list = generate_run(tfidf_results)\n",
    "\n",
    "with open(\"trec_eval_bm25_run_file.txt\", \"w\") as f:\n",
    "    for line in trec_bm25_run_list:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open(\"trec_eval_tfidf_run_file.txt\", \"w\") as f:\n",
    "    for line in trec_tfidf_run_list:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Qrels File generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1729868804818,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "BbLdcjrCHcdp"
   },
   "outputs": [],
   "source": [
    "# Create format for Trec_Eval\n",
    "qrels_file = []\n",
    "for qrel in trec_dl_2020.qrels_iter():\n",
    "    line = f\"{qrel.query_id} 0 {qrel.doc_id} {qrel.relevance}\"\n",
    "    qrels_file.append(line)\n",
    "\n",
    "with open(\"trec_eval_qrels_file.txt\", \"w\") as f:\n",
    "    for line in qrels_file:\n",
    "        f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1729868809424,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "zWWbnIHvHcdp",
    "outputId": "f0371710-d52c-4f93-92b6-dff784e0a672"
   },
   "outputs": [],
   "source": [
    "measures = [P@5, P(rel=2)@5, nDCG@10, AP, AP(rel=2), Bpref, Bpref(rel=2), Judged@10]\n",
    "\n",
    "qrels = ir_measures.read_trec_qrels('trec_eval_qrels_file.txt')\n",
    "bm25_run = ir_measures.read_trec_run(('trec_eval_bm25_run_file.txt'))\n",
    "bm25_results = ir_measures.calc_aggregate(measures, qrels, bm25_run)\n",
    "\n",
    "qrels = ir_measures.read_trec_qrels('trec_eval_qrels_file.txt')\n",
    "tfidf_run = ir_measures.read_trec_run(('trec_eval_tfidf_run_file.txt'))\n",
    "tfidf_results = ir_measures.calc_aggregate(measures, qrels, tfidf_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1729868809424,
     "user": {
      "displayName": "DANIEL NAMAKI GHANEH",
      "userId": "00865735165124121752"
     },
     "user_tz": -120
    },
    "id": "zWWbnIHvHcdp",
    "outputId": "f0371710-d52c-4f93-92b6-dff784e0a672"
   },
   "outputs": [],
   "source": [
    "bm25_results = ir_measures.calc_aggregate(measures, qrels, bm25_run)\n",
    "tfidf_results = ir_measures.calc_aggregate(measures, qrels, tfidf_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "df = pd.DataFrame({\n",
    "    \"BM25\": bm25_results,\n",
    "    \"TF-IDF\": tfidf_results\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mircv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
