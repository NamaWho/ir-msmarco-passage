{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval project \n",
    "**Authors:** Arduini L., Menchini L., Namaki Ghaneh D., Petruzzella C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ir_datasets in ./mircv_env/lib/python3.12/site-packages (0.5.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (4.12.3)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (2.5.0)\n",
      "Requirement already satisfied: lxml>=4.5.2 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.1 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (2.1.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.38.0 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (4.66.5)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (4.3.3)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (0.2.5)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (0.1.9)\n",
      "Requirement already satisfied: ijson>=3.1.3 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (3.3.0)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in ./mircv_env/lib/python3.12/site-packages (from ir_datasets) (0.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./mircv_env/lib/python3.12/site-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./mircv_env/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./mircv_env/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./mircv_env/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./mircv_env/lib/python3.12/site-packages (from requests>=2.22.0->ir_datasets) (2024.8.30)\n",
      "Requirement already satisfied: cbor>=1.0.0 in ./mircv_env/lib/python3.12/site-packages (from trec-car-tools>=2.5.4->ir_datasets) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m151.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in ./mircv_env/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m226.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.7/284.7 kB\u001b[0m \u001b[31m254.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ir_datasets\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "\n",
    "# Load the MS MARCO dataset\n",
    "dataset = ir_datasets.load(\"msmarco-passage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Treatments done by Medical Tourists in Costa Rica. Known initially for its excellent dental surgery services, medical tourism in Costa Rica has spread to a variety of other medical procedures, including: General and cosmetic dentistry; Cosmetic surgery; Aesthetic procedures (botox, skin resurfacing etc) Bariatric and Laparoscopic surgery\n"
     ]
    }
   ],
   "source": [
    "# print the first document in the dataset\n",
    "\n",
    "import random\n",
    "\n",
    "# Initialize a flag to check if a document has been printed\n",
    "document_printed = False\n",
    "\n",
    "# Iterate over the documents in the dataset\n",
    "for doc in dataset.docs_iter():\n",
    "    if not document_printed:\n",
    "        if random.random() < 0.01:  # Adjust the probability as needed\n",
    "            print(doc.text)\n",
    "            document_printed = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "def preprocess(s):\n",
    "    # lowercasing\n",
    "    s = s.lower()\n",
    "    # ampersand\n",
    "    s = s.replace(\"&\", \" and \")\n",
    "    # special chars\n",
    "    s = s.translate(dict([(ord(x), ord(y)) for x, y in zip(\"‘’´“”–-\", \"'''\\\"\\\"--\")]))\n",
    "    # acronyms\n",
    "    s = re.sub(r\"\\.(?!(\\S[^. ])|\\d)\", \"\", s)\n",
    "    # remove punctuation\n",
    "    s = s.translate(str.maketrans(string.punctuation, \" \" * len(string.punctuation)))\n",
    "    # strip whitespaces\n",
    "    s = s.strip()\n",
    "    while \"  \" in s:\n",
    "        s = s.replace(\"  \", \" \")\n",
    "    # tokeniser\n",
    "    s = s.split()\n",
    "    # stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    s = [t for t in s if t not in stopwords]\n",
    "    # stemming\n",
    "    stemmer = nltk.stem.PorterStemmer().stem\n",
    "    s = [stemmer(t) for t in s]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def profile(f):\n",
    "    def f_timer(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        ms = (end - start) * 1000\n",
    "        print(f\"{f.__name__} ({ms:.3f} ms)\")\n",
    "        return result\n",
    "    return f_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielnamaki/Documents/University/ir-project/mircv_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@profile\n",
    "def build_index(dataset):\n",
    "    lexicon = {}\n",
    "    doc_index = []\n",
    "    inv_d, inv_f = {}, {}\n",
    "    termid = 0\n",
    "\n",
    "    num_docs = 0\n",
    "    total_dl = 0\n",
    "    total_toks = 0\n",
    "    for docid, doc in tqdm(enumerate(dataset.docs_iter()), desc='Indexing', total=dataset.docs_count()):\n",
    "        tokens = preprocess(doc.text)\n",
    "        token_tf = Counter(tokens)\n",
    "        for token, tf in token_tf.items():\n",
    "            if token not in lexicon:\n",
    "                lexicon[token] = [termid, 0, 0]\n",
    "                inv_d[termid], inv_f[termid] =  [], []\n",
    "                termid += 1\n",
    "            token_id = lexicon[token][0]\n",
    "            inv_d[token_id].append(docid)\n",
    "            inv_f[token_id].append(tf)\n",
    "            lexicon[token][1] += 1\n",
    "            lexicon[token][2] += tf\n",
    "        doclen = len(tokens)\n",
    "        doc_index.append((str(doc.doc_id), doclen))\n",
    "        total_dl += doclen\n",
    "        num_docs += 1\n",
    "\n",
    "\n",
    "    stats = {\n",
    "        'num_docs': 1 + docid,\n",
    "        'num_terms': len(lexicon),\n",
    "        'num_tokens': total_dl,\n",
    "    }\n",
    "    return lexicon, {'docids': inv_d, 'freqs': inv_f}, doc_index, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing:  20%|██        | 1806321/8841823 [15:02<58:35, 2001.43it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lex, inv, doc, stats \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m, in \u001b[0;36mprofile.<locals>.f_timer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_timer\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      5\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m     ms \u001b[38;5;241m=\u001b[39m (end \u001b[38;5;241m-\u001b[39m start) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "Cell \u001b[0;32mIn[25], line 15\u001b[0m, in \u001b[0;36mbuild_index\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     13\u001b[0m total_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m docid, doc \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mdocs_iter()), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndexing\u001b[39m\u001b[38;5;124m'\u001b[39m, total\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mdocs_count()):\n\u001b[0;32m---> 15\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     token_tf \u001b[38;5;241m=\u001b[39m Counter(tokens)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token, tf \u001b[38;5;129;01min\u001b[39;00m token_tf\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# stopwords\u001b[39;00m\n\u001b[1;32m     25\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m s \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m s \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# stemming\u001b[39;00m\n\u001b[1;32m     28\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;241m.\u001b[39mPorterStemmer()\u001b[38;5;241m.\u001b[39mstem\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lex, inv, doc, stats = build_index(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class InvertedIndex:\n",
    "\n",
    "    class PostingListIterator:\n",
    "        def __init__(self, docids, freqs, doc):\n",
    "            self.docids = docids\n",
    "            self.freqs = freqs\n",
    "            self.pos = 0\n",
    "            self.doc = doc\n",
    "\n",
    "        def docid(self):\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "            return self.docids[self.pos]\n",
    "\n",
    "        def score(self):\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "            return self.freqs[self.pos]/self.doc[self.docid()][1]\n",
    "\n",
    "        def next(self, target = None):\n",
    "            if not target:\n",
    "                if not self.is_end_list():\n",
    "                    self.pos += 1\n",
    "            else:\n",
    "                if target > self.docid():\n",
    "                    try:\n",
    "                        self.pos = self.docids.index(target, self.pos)\n",
    "                    except ValueError:\n",
    "                        self.pos = len(self.docids)\n",
    "\n",
    "        def is_end_list(self):\n",
    "            return self.pos == len(self.docids)\n",
    "\n",
    "\n",
    "        def len(self):\n",
    "            return len(self.docids)\n",
    "\n",
    "\n",
    "    def __init__(self, lex, inv, doc, stats):\n",
    "        self.lexicon = lex\n",
    "        self.inv = inv\n",
    "        self.doc = doc\n",
    "        self.stats = stats\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.stats['num_docs']\n",
    "\n",
    "    def get_posting(self, termid):\n",
    "        return InvertedIndex.PostingListIterator(self.inv['docids'][termid], self.inv['freqs'][termid], self.doc)\n",
    "\n",
    "    def get_termids(self, tokens):\n",
    "        return [self.lexicon[token][0] for token in tokens if token in self.lexicon]\n",
    "\n",
    "    def get_postings(self, termids):\n",
    "        return [self.get_posting(termid) for termid in termids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index = InvertedIndex(lex, inv, doc, stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress and save the index components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "with gzip.open('lexicon.pickle.gz', 'wb') as f:\n",
    "  pickle.dump(lex, f)\n",
    "with gzip.open('inverted_file.pickle.gz', 'wb') as f:\n",
    "  pickle.dump(inv, f)\n",
    "with gzip.open('document_index.pickle.gz', 'wb') as f:\n",
    "  pickle.dump(doc, f)\n",
    "with gzip.open('stats.pickle.gz', 'wb') as f:\n",
    "  pickle.dump(stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Query processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mircv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
